#============================================
#
# 
# 
#             OUTDATED FILE
#
#
# 
#============================================

import os
import gc
import sys
import pickle
import random
import logging
from typing import List, Tuple, Generator, Optional

import numpy as np
import torch
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed

# Assuming your helper functions are in a 'pp' package
# Make sure these functions are robust and don't have their own memory leaks.
from pp.helpers import count_measures, extract_measure, split_midi_by_track_named, midi_to_velocity_matrix

# --- Configuration ---
# Use a hidden directory for temporary files generated by worker processes
TEMP_OUTPUT_DIR = "./outputs/.tmp_results"
FINAL_OUTPUT_DIR = "./outputs"
# The target size for each final consolidated .pt file
MAX_FILE_SIZE_BYTES = 500 * 1024 * 1024  # 500 MB
LOG_FILE = "processing.log"

# --- Logging Setup ---
# A more robust logging setup that also prints to console for immediate feedback
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(processName)s] %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, mode='w'),
        logging.StreamHandler(sys.stdout) # Also print logs to the console
    ]
)

# --- Core Data Processing Logic ---

def data_from_song_pipeline(input_file: Optional[str] = None, input_midi=None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
    """
    Processes a single MIDI file and yields (non_drum_matrix, drum_matrix) pairs.
    This is a GENERATOR, yielding pairs one by one to keep memory usage minimal.

    The pipeline steps are:
    1. Count measures in the song.
    2. For each measure:
       a. Extract the measure as a new MIDI object.
       b. Split the measure into instrument tracks and a drum track.
       c. If a drum track exists, convert it and each instrument track to velocity matrices.
       d. Yield a (non_drum_matrix, drum_matrix) pair for each valid instrument.
    """
    try:
        num_measures = count_measures(input_file=input_file, input_midi=input_midi)
        if num_measures is None or num_measures == 0:
            return
    except Exception as e:
        logging.warning(f"Could not count measures for {input_file or 'midi_object'}. Skipping. Error: {e}")
        return

    # Use a fixed dimension for the velocity matrix
    matrix_dim_x = 64

    for measure_num in range(1, num_measures + 1):
        try:
            measure_midi = extract_measure(measure_num, input_file=input_file, input_midi=input_midi)
            if not measure_midi:
                continue

            split_tracks = split_midi_by_track_named(input_midi=measure_midi)
            if not split_tracks or split_tracks[-1] is None:
                continue  # Skip measures without a drum track

            drum_track_midi = split_tracks[-1]
            drum_matrix = midi_to_velocity_matrix(input_midi=drum_track_midi, x=matrix_dim_x)

            if drum_matrix is None:
                continue

            for instrument_track_midi in split_tracks[:-1]:
                if instrument_track_midi is None:
                    continue

                inst_matrix = midi_to_velocity_matrix(input_midi=instrument_track_midi, x=matrix_dim_x)

                if inst_matrix is not None and inst_matrix.size > 0:
                    yield (inst_matrix, drum_matrix)

        except Exception as e:
            # Log specific measure errors but continue with the song
            logging.debug(f"Skipping measure {measure_num} in {input_file or 'midi_object'} due to error: {e}")
            continue

def process_artist_and_save(artist_path: str, temp_output_dir: str) -> Tuple[str, int, Optional[str]]:
    """
    Worker function: Processes all MIDI files for a single artist.
    Instead of returning data, it saves the result to a temporary file.
    Returns a status tuple: (artist_name, number_of_pairs, path_to_temp_file).
    """
    artist_name = os.path.basename(artist_path)
    all_artist_pairs = []

    midi_files = [f for f in os.listdir(artist_path) if f.lower().endswith(('.mid', '.midi'))]
    if not midi_files:
        return artist_name, 0, None

    logging.info(f"Processing artist: {artist_name}")

    for filename in midi_files:
        file_path = os.path.join(artist_path, filename)
        try:
            # The generator is exhausted into a list here, but only for one song at a time.
            song_pairs = list(data_from_song_pipeline(input_file=file_path))
            if song_pairs:
                all_artist_pairs.extend(song_pairs)
        except Exception as e:
            logging.error(f"Failed to process song {file_path}. Error: {e}")

    # After processing all songs for the artist, save the result if any data was generated.
    num_pairs = len(all_artist_pairs)
    if num_pairs == 0:
        logging.info(f"No valid data found for artist: {artist_name}")
        return artist_name, 0, None

    # Save the collected pairs to a unique temporary file
    temp_file_path = os.path.join(temp_output_dir, f"{artist_name}.pt")
    try:
        torch.save(all_artist_pairs, temp_file_path)
        logging.info(f"Saved {num_pairs} pairs for artist {artist_name} to {temp_file_path}")
        # Return the path to the temp file for later consolidation
        return artist_name, num_pairs, temp_file_path
    except Exception as e:
        logging.error(f"Could not save temporary file for artist {artist_name}. Error: {e}")
        return artist_name, num_pairs, None
    finally:
        # Explicitly clear the large list and trigger garbage collection
        del all_artist_pairs
        gc.collect()


def consolidate_output_files(temp_files: List[str], final_dir: str, max_size_bytes: int):
    """
    Reads all temporary .pt files, consolidates them into larger files of
    a specified max size, and then deletes the temporary files.
    """
    if not temp_files:
        logging.warning("No temporary files to consolidate.")
        return

    logging.info(f"Consolidating {len(temp_files)} temporary files into larger chunks...")

    os.makedirs(final_dir, exist_ok=True)
    
    current_data_batch = []
    current_size_bytes = 0
    file_index = 1

    # Find the next available file index in the final directory
    existing_files = [f for f in os.listdir(final_dir) if f.endswith('.pt')]
    if existing_files:
        indices = [int(f.split('.')[0]) for f in existing_files if f.split('.')[0].isdigit()]
        if indices:
            file_index = max(indices) + 1


    for temp_file_path in tqdm(temp_files, desc="Consolidating"):
        try:
            data = torch.load(temp_file_path)
            current_data_batch.extend(data)
            os.remove(temp_file_path) # Delete temp file after loading

            # Check size to see if we need to save a chunk
            # Using pickle to get a realistic size estimate, similar to the original script
            serialized_size = len(pickle.dumps(current_data_batch))

            if serialized_size >= max_size_bytes:
                save_path = os.path.join(final_dir, f"{file_index:05d}.pt")
                torch.save(current_data_batch, save_path)
                logging.info(f"Saved consolidated file: {save_path} ({serialized_size / (1024*1024):.2f} MB)")

                # Reset for the next batch
                file_index += 1
                current_data_batch = []
                gc.collect()

        except Exception as e:
            logging.error(f"Error consolidating {temp_file_path}. Skipping. Error: {e}")
            continue

    # Save any remaining data that didn't fill a full chunk
    if current_data_batch:
        save_path = os.path.join(final_dir, f"{file_index:05d}.pt")
        torch.save(current_data_batch, save_path)
        final_size_mb = len(pickle.dumps(current_data_batch)) / (1024*1024)
        logging.info(f"Saved final remaining batch: {save_path} ({final_size_mb:.2f} MB)")

    logging.info("Consolidation complete.")

def generate_dataset(dataset_path: str, max_workers: Optional[int] = None):
    """
    Main function to run the entire dataset generation pipeline.
    1. Sets up directories.
    2. Uses a ProcessPoolExecutor to process each artist in parallel.
    3. Each worker saves its result to a temporary file.
    4. After all workers complete, consolidates the temporary files.
    """
    logging.info("Starting dataset generation pipeline...")
    os.makedirs(TEMP_OUTPUT_DIR, exist_ok=True)
    os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)

    artist_folders = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
    if not artist_folders:
        logging.error(f"No artist subdirectories found in {dataset_path}. Aborting.")
        return

    total_artists = len(artist_folders)
    temp_files_to_consolidate = []
    total_pairs_generated = 0

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Create a future for each artist processing task
        future_to_artist = {
            executor.submit(process_artist_and_save, os.path.join(dataset_path, artist), TEMP_OUTPUT_DIR): artist
            for artist in artist_folders
        }

        # Process results as they complete
        progress_bar = tqdm(as_completed(future_to_artist), total=total_artists, desc="Processing Artists")
        for future in progress_bar:
            artist_name = future_to_artist[future]
            try:
                name, num_pairs, temp_file_path = future.result()
                if temp_file_path:
                    temp_files_to_consolidate.append(temp_file_path)
                    total_pairs_generated += num_pairs
                progress_bar.set_postfix_str(f"Last: {name} ({num_pairs} pairs)")
            except Exception as e:
                logging.error(f"Artist '{artist_name}' processing failed with an unexpected error: {e}")

    logging.info(f"\n--- Parallel processing complete ---")
    logging.info(f"Total artists processed: {total_artists}")
    logging.info(f"Total data pairs generated: {total_pairs_generated}")
    logging.info(f"Temporary files created: {len(temp_files_to_consolidate)}")

    # --- Step 2: Consolidate all the temporary files ---
    consolidate_output_files(
        temp_files=temp_files_to_consolidate,
        final_dir=FINAL_OUTPUT_DIR,
        max_size_bytes=MAX_FILE_SIZE_BYTES
    )

    # Clean up the temporary directory if it's empty
    try:
        if not os.listdir(TEMP_OUTPUT_DIR):
            os.rmdir(TEMP_OUTPUT_DIR)
    except OSError as e:
        logging.warning(f"Could not remove temporary directory {TEMP_OUTPUT_DIR}. You may need to remove it manually. Error: {e}")

    logging.info("✅✅✅ Dataset generation finished successfully! ✅✅✅")


if __name__ == "__main__":
    # Set the path to your dataset of MIDI files, organized by artist folders
    # e.g., ./clean_midi/Artist_Name/song.mid
    MIDI_DATASET_PATH = "./clean_midi"
    
    # You can limit the number of parallel workers to control CPU/RAM usage.
    # If None, it defaults to the number of CPUs on your machine.
    # For memory-intensive tasks, it's often wise to use fewer workers than cores.
    # For a 32GB system, 4-8 workers is a reasonable starting point.
    NUM_WORKERS = os.cpu_count() // 2 if os.cpu_count() else 4

    generate_dataset(dataset_path=MIDI_DATASET_PATH, max_workers=NUM_WORKERS)